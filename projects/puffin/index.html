<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation | ICLR 2026</title>
    <meta name="description" content="We make the first attempt to unify camera-centric understanding and generation in a cohesive multimodal framework.">
    <meta name="keywords" content="Spatial Intelligence, Unified Multimodal Model, Camera-Centric Understanding and Generation">
    <meta name="author" content="Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://kangliao929.github.io/projects/puffin">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://kangliao929.github.io/projects/puffin">
    <meta property="og:title" content="Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation">
    <meta property="og:description" content="We make the first attempt to unify camera-centric understanding and generation in a cohesive multimodal framework.">
    <meta property="og:image" content="imgs/Puffin.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:site_name" content="Puffin Project">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://kangliao929.github.io/projects/puffin">
    <meta property="twitter:title" content="Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation">
    <meta property="twitter:description" content="We make the first attempt to unify camera-centric understanding and generation in a cohesive multimodal framework.">
    <meta property="twitter:image" content="imgs/Puffin.png">
    
    <!-- Structured Data - Research Paper -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "ScholarlyArticle",
      "headline": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation",
      "description": "We make the first attempt to unify camera-centric understanding and generation in a cohesive multimodal framework.",
      "author": [
        {
          "@type": "Person",
          "name": "Kang Liao",
          "url": "https://kangliao929.github.io/"
        },
        {
          "@type": "Person",
          "name": "Size Wu",
          "url": "https://wusize.github.io/"
        },
        {
          "@type": "Person",
          "name": "Zhonghua Wu",
          "url": "https://wu-zhonghua.github.io/"
        },
        {
          "@type": "Person",
          "name": "Linyi Jin",
          "url": "https://jinlinyi.github.io/"
        },
        {
          "@type": "Person",
          "name": "Chao Wang",
          "url": "https://hans1984.github.io/"
        },
        {
          "@type": "Person",
          "name": "Yikai Wang",
          "url": "https://yikai-wang.github.io/"
        },
        {
          "@type": "Person",
          "name": "Fei Wang",
          "url": "https://scholar.google.com/citations?user=ljt16JkAAAAJ&hl"
        },
        {
          "@type": "Person",
          "name": "Wei Li",
          "url": "https://weivision.github.io/"
        },
        {
          "@type": "Person",
          "name": "Chen Change Loy",
          "url": "https://www.mmlab-ntu.com/person/ccloy/index.html"
        }
      ],
      "publisher": {
        "@type": "Organization",
        "name": "S-Lab, Nanyang Technological University"
      },
      "datePublished": "2026",
      "isPartOf": {
        "@type": "PublicationIssue",
        "isPartOf": {
          "@type": "PublicationVolume",
          "isPartOf": {
            "@type": "Periodical",
            "name": "ICLR 2026"
          }
        }
      },
      "url": "https://kangliao929.github.io/projects/puffin",
      "sameAs": "http://arxiv.org/"
    }
    
    </script>
    
    <!-- Theme Color -->
    <meta name="theme-color" content="#ffffff">
    
    <!-- Bootstrap CSS-->
    <link rel="stylesheet" href="vendor/bootstrap/css/bootstrap.min.css">
    <!-- Font Awesome CSS-->
    <link rel="stylesheet" href="vendor/font-awesome/css/font-awesome.min.css">
    <!-- Google fonts - Roboto-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,700&display=swap">
    <!-- Bootstrap Select-->
    <link rel="stylesheet" href="vendor/bootstrap-select/css/bootstrap-select.min.css">
    <!-- owl carousel-->
    <link rel="stylesheet" href="vendor/owl.carousel/assets/owl.carousel.css">
    <link rel="stylesheet" href="vendor/owl.carousel/assets/owl.theme.default.css">
    <!-- theme stylesheet-->
    <link rel="stylesheet" href="css/style.oxfordblue.css" id="theme-stylesheet">
    <!-- model-viewer -->
    <link rel="stylesheet" href="css/model-viewer.css">
    <!-- Custom stylesheet - for your changes-->
    <link rel="stylesheet" href="css/custom.css">
    <!-- Favicon and apple touch icons-->
    <link rel="icon" type="image/png" href="imgs/Puffin_logo.png" sizes="96x88" />
    <link rel="icon" type="image/svg+xml" href="imgs/Puffin_logo.svg" />
    <link rel="shortcut icon" href="imgs/Puffin_logo.ico" />
    <link rel="apple-touch-icon" sizes="180x168" href="imgs/Puffin_logo.png" />
    <link rel="manifest" href="site.webmanifest" />
    <!-- Tweaks for older IEs--><!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script><![endif]-->
    <style>
      #demo-container {
        width: 100%;
        height: 500px;
      }

      .video-wrapper {
        position: relative;
        display: inline-block;
        overflow: visible;
      }

      .overlay-badge {
        position: absolute;
        bottom: -10px;   
        right: -20px;
        width: 64px;
        height: 48px;
        pointer-events: none;
        z-index: 10;
      }
      #thumbnailCarousel {

        width: max-content;     
        margin-left: auto;      
        margin-right: auto;
      }

      .loading-spinner {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        z-index: 20;
        display: none;
      }

      .spinner {
        border: 4px solid #f3f3f3;
        border-top: 4px solid #007bff;
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
      }

      @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
      }

    </style>
  </head>
  <body>
    <div id="all">

      <main>
      <section class="bar bg-white no-mb padding-small">
        <div data-animate="fadeInUpBig" class="container">
          <div class="row">
            <div class="col-md-12">
              <header class="heading-light text-center mb-medium">
                <h1>
                  <img src="imgs/Puffin.png" alt="Puffin" class="img-fluid d-inline-block align-middle" style="height: 3em; margin-right: 0px; margin-top: -0.46em;" loading="eager">Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation
                </h1>
              </header>
              <p class="lead text-md-center">
                <a href="https://kangliao929.github.io/">Kang Liao<sup>1</sup></a>,
                <a href="https://wusize.github.io/">Size Wu<sup>1</sup></a>,
                <a href="https://wu-zhonghua.github.io/">Zhonghua Wu<sup>2</sup></a>,
                <a href="https://jinlinyi.github.io/">Linyi Jin<sup>3</sup></a>,
                <br>
                <a href="https://hans1984.github.io/">Chao Wang<sup>4</sup></a>,
                <a href="https://yikai-wang.github.io/">Yikai Wang<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=ljt16JkAAAAJ&hl">Fei Wang<sup>2</sup></a>,
                <a href="https://weivision.github.io/">Wei Li<sup>1</sup></a>,
                <a href="https://www.mmlab-ntu.com/person/ccloy/index.html">Chen Change Loy<sup>1</sup></a>
                <br>
                <small>
                  <sup>1</sup>S-Lab, Nanyang Technological University <sup>2</sup>SenseTime Research 
                  <br>
                  <sup>3</sup>University of Michigan <sup>4</sup>Max-Planck Institute for Informatics
                </small>
              </p>
              <p class="lead text-md-center">
                <strong>ICLR 2026</strong>
              </p>
              <div class="see-more text-center">
                <p class="buttons lead">
                  <a href="https://arxiv.org/abs/2510.08673" class="btn btn-template-outlined"><i class="fa fa-file-text-o"></i> Paper</a>
                  <a href="https://github.com/KangLiao929/Puffin" class="btn btn-template-outlined"><i class="fa fa-github"></i> Code</a>
                  <a href="https://huggingface.co/KangLiao/Puffin" class="btn btn-template-outlined"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" style="height: 1em; margin-right: 5px;">Model</a>
                  <a href="https://huggingface.co/datasets/KangLiao/Puffin-4M" class="btn btn-template-outlined"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" style="height: 1em; margin-right: 5px;">Dataset</a>
                  <a href="https://huggingface.co/spaces/KangLiao/Puffin" class="btn btn-template-outlined"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" style="height: 1em; margin-right: 5px;">Demo</a>
                  <a href="supplemental.html" class="btn btn-template-outlined"><i class="fa fa-link"></i> Supplementary Results</a>
                </p>
                </p>
              </div>

            </div>
          </div>
        </div>
      </section>


      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2></h2>
              <img src="imgs/tesear.png" alt="Teaser" class="img-fluid image1 mb-small" loading="lazy" style="width:70%; height:auto; display:block; margin:auto;">

              <div style="width:75%; margin:auto;">
                <p class="lead mb-small"> 
                  <b>Illustration of the versatile capabilities of our model.</b> It unifies multimodal camera-centric generation (a) and understanding (b), supports the thinking mode (c), and enables diverse cross-view applications (d).
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="container">
       
      </section>

      <section class="bar bg-white padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Quick Explanation</h2>

              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video autoplay loop playsinline inline controls>
                  <source src="imgs/quick_explanation.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Framework</h2>
              <img src="imgs/crop_framework.png" alt="Framework" class="img-fluid image1 mb-small" loading="lazy">

              <p class="lead mb-small"> 
                We present <b>Puffin</b>, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. It learns the camera-centric understanding and generation tasks in a unified multimodal framework. The elements bounded with dotted boundaries represent the cross-view understanding and generation during instruction tuning, such as spatial imagination and world exploration.
              </p>
            </div>
          </div>
        </div>
        <div class="container">
       
      </section>

      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Thinking with Camera</h2>

              <img src="imgs/thinking.png" alt="Thinking with Camera" class="img-fluid image1 mb-small" loading="lazy">
              <p class="lead mb-small"> 
              Previous methods focused on extracting or learning <em>representations</em> such as geometric structures or semantic features (with confidence) to estimate camera geometry from images. We introduce the notion of <em>thinking with camera</em> through LMMs. It decouples the camera parameters across geometric context, establishing connections between spatially grounded visual cues (highlighted in the masked regions) and professional photographic terms. The camera parameters are then predicted within the <strong>&lt;answer&gt;&lt;/answer&gt;</strong> tag (in radius) through this spatial reasoning process <strong>&lt;think&gt;&lt;/think&gt;</strong>. Please select the following samples to visualize different thinking processes.
              </p>
            </div>
          </div>
        </div>
      </section>


        <style>
        .video-section {
          display: flex;
          flex-direction: column;
          align-items: center;
          gap: 0.5rem;
          text-align: center;
        }
        .video-wrapper {
          position: relative;
          display: inline-block;
          line-height: 0;
        }
        .video-wrapper video {
          display: block;
          width: auto !important;
          height: auto !important;
          max-width: 100%;
          border-radius: .25rem;
          box-shadow: 0 .125rem .25rem rgba(0,0,0,.075);
        }

        .loading-spinner {
          position: absolute;
          inset: 0;
          display: none;
          align-items: center;
          justify-content: center;
          background: rgba(255,255,255,0.0);
          pointer-events: none;
        }
        .loading-spinner .spinner {
          width: 40px;
          height: 40px;
          border: 4px solid rgba(0,0,0,0.15);
          border-top-color: rgba(0,0,0,0.6);
          border-radius: 50%;
          animation: spin 0.8s linear infinite;
        }
        @keyframes spin {
          to { transform: rotate(360deg); }
        }

        #thumbnailCarousel .thumb {
          height: 80px;
          cursor: pointer;
          object-fit: cover;
        }
        #thumbnailCarousel .thumb.active {
          outline: 2px solid #ffc107;
        }
      </style>

      <div class="container">
        <div class="row mb-4">
          <div class="col-12">
            <div class="video-section">
              <div class="video-wrapper" aria-live="polite">
                <video id="generatedVideo" autoplay loop muted playsinline
                      preload="auto">
                  <source src="imgs/think_cam1.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>

                <div id="genVideoSpinner" class="loading-spinner">
                  <div class="spinner"></div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="row">
          <div class="col-12">
            <div class="d-flex flex-wrap justify-content-center gap-2 py-3 px-3 bg-light rounded" id="thumbnailCarousel">
              <img src="imgs/think_cam1.png"
                  data-img="imgs/think_cam1.png"
                  data-video="imgs/think_cam1.mp4"
                  class="thumb active border border-warning rounded">
              <img src="imgs/think_cam2.png"
                  data-img="imgs/think_cam2.png"
                  data-video="imgs/think_cam2.mp4"
                  class="thumb border rounded">
              <img src="imgs/think_cam3.png"
                  data-img="imgs/think_cam3.png"
                  data-video="imgs/think_cam3.mp4"
                  class="thumb border rounded">
              <img src="imgs/think_cam4.png"
                  data-img="imgs/think_cam4.png"
                  data-video="imgs/think_cam4.mp4"
                  class="thumb border rounded">
              <img src="imgs/think_cam5.png"
                  data-img="imgs/think_cam5.png"
                  data-video="imgs/think_cam5.mp4"
                  class="thumb border rounded">
            </div>
          </div>
        </div>
      </div>

      <script>
        const thumbnails       = document.querySelectorAll('#thumbnailCarousel img');
        const genVideo         = document.getElementById('generatedVideo');
        const genVideoSpinner  = document.getElementById('genVideoSpinner');

        let genVideoTimeout = null;

        function showSpinnerDelayed(spinner, timeoutRef, delay = 300) {
          clearTimeout(timeoutRef);
          return setTimeout(() => {
            spinner.style.display = 'flex';
          }, delay);
        }

        function hideSpinner(spinner, timeoutRef) {
          clearTimeout(timeoutRef);
          spinner.style.display = 'none';
        }

        genVideoTimeout = showSpinnerDelayed(genVideoSpinner, genVideoTimeout);

        genVideo.addEventListener('loadstart', () => {
          genVideoTimeout = showSpinnerDelayed(genVideoSpinner, genVideoTimeout);
        });
        genVideo.addEventListener('waiting', () => {
          genVideoTimeout = showSpinnerDelayed(genVideoSpinner, genVideoTimeout);
        });
        genVideo.addEventListener('playing', () => hideSpinner(genVideoSpinner, genVideoTimeout));
        genVideo.addEventListener('loadeddata', () => hideSpinner(genVideoSpinner, genVideoTimeout));
        genVideo.addEventListener('error', () => hideSpinner(genVideoSpinner, genVideoTimeout));

        let syncInProgress = false;
        function syncVideos(sourceVideo, targetVideo) {
          if (syncInProgress) return;
          syncInProgress = true;

          targetVideo.currentTime = sourceVideo.currentTime;
          if (!sourceVideo.paused) {
            targetVideo.play().catch(() => {});
          } else {
            targetVideo.pause();
          }

          setTimeout(() => { syncInProgress = false; }, 100);
        }

        thumbnails.forEach(thumb => {
          thumb.addEventListener('click', () => {
            thumbnails.forEach(t => t.classList.remove('active', 'border-warning'));
            thumb.classList.add('active', 'border-warning');

            const srcEl = genVideo.querySelector('source');
            if (srcEl && thumb.dataset.video) {
              srcEl.src = thumb.dataset.video;
              genVideo.load();
            }
          });
        });
      </script>



      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Puffin-4M Dataset</h2>

              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video loop muted playsinline inline controls>
                  <source src="imgs/dataset.mp4" type="video/mp4">
                </video>
              </div>
              <p class="lead mb-small"> 
              Datasets and benchmarks that span vision, language, and camera modalities remain scarce in the domain of spatial multimodal intelligence. To address this gap, we introduce <b>Puffin-4M</b>, a large-scale, high-quality dataset comprising 4 million vision-language-camera triplets. The pipeline of the dataset construction is illustrated as follows.
              </p>
              <img src="imgs/dataset_pipeline.png" alt="Dataset Pipeline" class="img-fluid image1 mb-small" loading="lazy">
              <p class="lead mb-small"> 
              The construction of this curated dataset consists of four stages: panoramic data collection and preprocessing, perspective image generation, scene and spatial reasoning captioning, and extensions for cross-view scenarios.
              </p>
            </div>
          </div>
        </div>
      </section>



      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Comparison Results</h2>
              <img src="imgs/qualitative_evaluation.png" alt="Qualitative Evaluation" class="img-fluid image1 mb-small" loading="lazy">
              <p class="lead mb-small"> 
                Qualitative comparison with the state-of-the-art camera-centric generation and understanding methods. For generated images, we adopt an offline camera calibration method to estimate their pixel-wise camera maps. We then compute the median errors of the up vector and latitude to evaluate the spatial simulation performance. For camera understanding, horizon lines are visualized from the estimated camera parameters and GT. Please refer to the Supplementary Results for more details (quantitative evaluation and more visualizations).
              </p>

              <div class="see-more text-center">
                <p class="buttons lead">
                  <a href="supplemental.html" class="btn btn-template-outlined"><i class="fa fa-link"></i> Supplementary Results</a>
                </p>
              </div>
          </div>
        </div>
      </section>



      <section class="bar bg-white padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2>Applications</h2>
              <img src="imgs/applications.png" alt="Applications" class="img-fluid image1 mb-small" loading="lazy">
              <p class="lead mb-small"> 
               Although Puffin primarily focuses on single-view camera understanding and text-to-image camera-controllable generation, it can be flexibly extended to cross-view settings with our <b>instruction tuning</b><sup>*</sup>, by appending additional tokens and switching prompts according to the target task.              
              </p>
              <h3><img src="imgs/world_exploration_logo.png" alt="Logo" style="height:3em; vertical-align:middle;"> World Exploration (Cross-View Generation)</h3>
              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video loop muted playsinline inline controls>
                  <source src="imgs/world_exploration.mp4" type="video/mp4">
                </video>
              </div>
              <p class="lead mb-small"> 
              Given an initial view (marked by red box) and expected camera motion (<img src="imgs/camera_motion.png" alt="Logo" style="height:1em; vertical-align:middle;">), Puffin can generate the target view with proper spatial consistency. The 3D reconstructed results are derived from the initial view and generated views using VGGT.
              </p>
              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video loop muted playsinline inline controls>
                  <source src="imgs/cross_view_gen_iter.mp4" type="video/mp4">
                </video>
              </div>
              <p class="lead mb-small"> 
              By iteratively feeding the generated views into the model, Puffin can generate long-range and panorama-like scenes.
              </p>
              <h3><img src="imgs/spatial_imagination_logo.png" alt="Logo" style="height:3em; vertical-align:middle;"> Spatial Imagination (Cross-View Understanding)</h3>
              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video loop muted playsinline inline controls>
                  <source src="imgs/spatial_imagination_tight.mp4" type="video/mp4">
                </video>
              </div>
              <p class="lead mb-small"> 
              Puffin can imagine and describe the scene of a target view given its orientation and an initial view.
              </p>
              <h3><img src="imgs/photographic_guidance_logo.png" alt="Logo" style="height:3em; vertical-align:middle;"> Photographic Guidance (Cross-View Understanding)</h3>
              <div class="img-fluid embed-responsive embed-responsive-16by9">
                <video loop muted playsinline inline controls>
                  <source src="imgs/photographic_guidance.mp4" type="video/mp4">
                </video>
              </div>
              <p class="lead mb-small"> 
              Puffin can suggest camera parameter adjustments from an initial view to achieve images with higher photographic aesthetics.
              </p>
            </div>
          </div>
        </div>
      </section>

      <section class="bar bg-white mb-0 padding-small">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <h2 class="no-mb">Paper</h2>
              <a href="https://arxiv.org/abs/2510.08673">
                <div class="box-image no-mt">
                  <img src="imgs/paper_montage.png" alt="Thinking with Camera" class="img-fluid image1" loading="lazy">
                  <div class="overlay d-flex align-items-center justify-content-center"><span class="btn btn-template-outlined-white"><i class="fa fa-chain"> </i> arXiv Preprint</a></div>
                </div>
              </a>
            </div>

            <div class="col-md-12">
              <h3 id="section-bibtex">BibTeX</h3>
<pre>
  @article{liao2025puffin,
    title={Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation},
    author={Liao, Kang and Wu, Size and Wu, Zhonghua and Jin, Linyi and Wang, Chao and Wang, Yikai and Wang, Fei and Li, Wei and Loy, Chen Change},
    journal={arXiv preprint arXiv:2510.08673},
    year={2025}
  }
</pre>
            </div>
          </div>
        </div>
      </section>


      <section class="bar bg-white no-mb padding-big text-center-sm filler">
      </section>
      </main>
      <!-- FOOTER -->
      <footer class="main-footer bg-white">
        <div class="copyrights bg-white">
          <div class="container">
            <div class="row">
              <div class="col-lg-4 text-center-md">
                <!-- <p>&copy; 2021. Tomas Jakab</p> -->
              </div>
              <div class="col-lg-8 text-right text-center-md">
                <p>Template design by <a href="https://bootstrapious.com/free-templates">Bootstrapious Templates </a> and <a href="https://v-mem.github.io/">VMem </a></p>
                <!-- Please do not remove the backlink to us unless you support further theme's development at https://bootstrapious.com/donate. It is part of the license conditions. Thank you for understanding :)-->
              </div>
            </div>
          </div>
        </div>
      </footer>
    </div>
    <!-- Javascript files-->
    <!-- Script for model-viewer helper-->
    <script src="js/model-viewer-helper.js"></script>
    <!-- Loads <model-viewer> for browsers: -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
    <!-- Bootstrap JS-->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/popper.js/umd/popper.min.js"> </script>
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>
    <script src="vendor/jquery.cookie/jquery.cookie.js"> </script>
    <script src="vendor/waypoints/lib/jquery.waypoints.min.js"> </script>
    <script src="vendor/jquery.counterup/jquery.counterup.min.js"> </script>
    <script src="vendor/owl.carousel/owl.carousel.min.js"></script>
    <script src="vendor/owl.carousel2.thumbs/owl.carousel2.thumbs.min.js"></script>
    <script src="js/jquery.parallax-1.1.3.js"></script>
    <script src="vendor/bootstrap-select/js/bootstrap-select.min.js"></script>
    <script src="vendor/jquery.scrollto/jquery.scrollTo.min.js"></script>
    <script src="js/front.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.4.0/dist/tf.min.js"></script>
  </body>
</html>
